{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            id                                               text author\n",
      "0      id26305  This process, however, afforded me no means of...    EAP\n",
      "1      id17569  It never once occurred to me that the fumbling...    HPL\n",
      "2      id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
      "3      id27763  How lovely is spring As we looked from Windsor...    MWS\n",
      "4      id12958  Finding nothing else, not even gold, the Super...    HPL\n",
      "5      id22965  A youth passed in solitude, my best years spen...    MWS\n",
      "6      id09674  The astronomer, perhaps, at this point, took r...    EAP\n",
      "7      id13515        The surcingle hung in ribands from my body.    EAP\n",
      "8      id19322  I knew that you could not say to yourself 'ste...    EAP\n",
      "9      id00912  I confess that neither the structure of langua...    MWS\n",
      "10     id16737  He shall find that I can feel my injuries; he ...    MWS\n",
      "11     id16607  Here we barricaded ourselves, and, for the pre...    EAP\n",
      "12     id19764  Herbert West needed fresh bodies because his l...    HPL\n",
      "13     id18886  The farm like grounds extended back very deepl...    HPL\n",
      "14     id17189   But a glance will show the fallacy of this idea.    EAP\n",
      "15     id12799  He had escaped me, and I must commence a destr...    MWS\n",
      "16     id08441  To these speeches they gave, of course, their ...    EAP\n",
      "17     id13117  Her native sprightliness needed no undue excit...    MWS\n",
      "18     id14862  I even went so far as to speak of a slightly h...    EAP\n",
      "19     id20836  His facial aspect, too, was remarkable for its...    HPL\n",
      "20     id11411  Now the net work was not permanently fastened ...    EAP\n",
      "21     id08075  It was not that the sounds were hideous, for t...    HPL\n",
      "22     id18925  On every hand was a wilderness of balconies, o...    EAP\n",
      "23     id19925  With how deep a spirit of wonder and perplexit...    EAP\n",
      "24     id01704  These bizarre attempts at explanation were fol...    EAP\n",
      "25     id10125  For many prodigies and signs had taken place, ...    EAP\n",
      "26     id02448  All that as yet can fairly be said to be known...    EAP\n",
      "27     id23451  I seemed to be upon the verge of comprehension...    EAP\n",
      "28     id27907  Our compasses, depth gauges, and other delicat...    HPL\n",
      "29     id08121  This the young warriors took back with them to...    HPL\n",
      "...        ...                                                ...    ...\n",
      "19549  id20955  But it was not so; I was the same in strength,...    MWS\n",
      "19550  id01270  He then took the book himself, and read me a c...    EAP\n",
      "19551  id22290  \"Adolphe Le Bon, clerk to Mignaud et Fils, dep...    EAP\n",
      "19552  id20272  But of the character of his remarks at the per...    EAP\n",
      "19553  id18082  He notes every variation of face as the play p...    EAP\n",
      "19554  id07976  They admitted they had been drunk, but both vo...    HPL\n",
      "19555  id26741  The rays of the newly risen sun poured in upon...    EAP\n",
      "19556  id26698  To the north on the craggy precipice a few pac...    EAP\n",
      "19557  id22265  The frauds of the banks of course I couldn't h...    EAP\n",
      "19558  id14778  He was attired, as I had expected, in a costum...    EAP\n",
      "19559  id18823  When a fumbling came in the nearer casements h...    HPL\n",
      "19560  id00893  But then there is the tone laconic, or curt, w...    EAP\n",
      "19561  id08678  Average people in society and business New Eng...    HPL\n",
      "19562  id10857  The modes and sources of this kind of error ar...    EAP\n",
      "19563  id10563  Yet from whom has not that rude hand rent away...    MWS\n",
      "19564  id11752  Almighty God no, no They heard they suspected ...    EAP\n",
      "19565  id26214  I hope you have not been so foolish as to take...    EAP\n",
      "19566  id00832  These reflections made our legislators pause, ...    MWS\n",
      "19567  id04187  Because there were some considerations of deep...    EAP\n",
      "19568  id22378  Before going in we walked up the street, turne...    EAP\n",
      "19569  id26790  Once my fancy was soothed with dreams of virtu...    MWS\n",
      "19570  id14263  Nay, you may have met with another whom you ma...    MWS\n",
      "19571  id14420  My watch was still going, and told me that the...    HPL\n",
      "19572  id03325  But these and other difficulties attending res...    EAP\n",
      "19573  id07567  Stress of weather drove us up the Adriatic Gul...    MWS\n",
      "19574  id17718  I could have fancied, while I looked at it, th...    EAP\n",
      "19575  id08973  The lids clenched themselves together as if in...    EAP\n",
      "19576  id05267  Mais il faut agir that is to say, a Frenchman ...    EAP\n",
      "19577  id17513  For an item of news like this, it strikes us i...    EAP\n",
      "19578  id00393  He laid a gnarled claw on my shoulder, and it ...    HPL\n",
      "\n",
      "[19579 rows x 3 columns]\n",
      "           id                                               text\n",
      "0     id02310  Still, as I urged our leaving Ireland with suc...\n",
      "1     id24541  If a fire wanted fanning, it could readily be ...\n",
      "2     id00134  And when they had broken down the frail door t...\n",
      "3     id27757  While I was thinking how I should possibly man...\n",
      "4     id04081  I am not sure to what limit his knowledge may ...\n",
      "5     id27337  \"The thick and peculiar mist, or smoke, which ...\n",
      "6     id24265  That which is not matter, is not at all unless...\n",
      "7     id25917  I sought for repose although I did not hope fo...\n",
      "8     id04951  Upon the fourth day of the assassination, a pa...\n",
      "9     id14549         \"The tone metaphysical is also a good one.\n",
      "10    id22505  These, the offspring of a later period, stood ...\n",
      "11    id24002  What kept him from going with her and Brown Je...\n",
      "12    id18982  Persuading the widow that my connexion with he...\n",
      "13    id15181  When I arose trembling, I know not how much la...\n",
      "14    id21888  And by the shores of the river Zaire there is ...\n",
      "15    id12035  Idris heard of her mother's return with pleasure.\n",
      "16    id17991  I say this proudly, but with tears in my eyes ...\n",
      "17    id10707  But let us glance at the treatise Ah \"Ability ...\n",
      "18    id07101  \"What a place is this that you inhabit, my son...\n",
      "19    id00345  At his nod I took one of the latter and seated...\n",
      "20    id05912  No one doubted now that the mystery of this mu...\n",
      "21    id13443  But although, in one or two instances, arrests...\n",
      "22    id09248  Festivity, and even libertinism, became the or...\n",
      "23    id17542        For I am Iranon, who was a Prince in Aira.\"\n",
      "24    id06995  \"Gaze not on the star, dear, generous friend,\"...\n",
      "25    id25159  I am serious in asserting that my breath was e...\n",
      "26    id25729  The thing will haunt me, for who can say the e...\n",
      "27    id26949  Before each of the party lay a portion of a sk...\n",
      "28    id27191  If she had been bred in that sphere of life to...\n",
      "29    id07668  Or, if this mode of speech offend you, let me ...\n",
      "...       ...                                                ...\n",
      "8362  id22510  Then again he distracted my thoughts from my s...\n",
      "8363  id19204  Upon the whole, whether happily or unhappily, ...\n",
      "8364  id05758  He was not allowed to finish this speech in tr...\n",
      "8365  id27063  His looks were wild with terror, and he spoke ...\n",
      "8366  id11773  By the quantity of provision which I had consu...\n",
      "8367  id11562  I hurled after the scoundrel these vehement wo...\n",
      "8368  id16208  Notwithstanding the hazardous object of our jo...\n",
      "8369  id04036  I felt the greatest eagerness to hear the prom...\n",
      "8370  id26159  But in the expression of the countenance, whic...\n",
      "8371  id26777  Its decorations were rich, yet tattered and an...\n",
      "8372  id08501  He directed my attention to some object agains...\n",
      "8373  id11216  Hey? Haow'd ye like to hear the haowlin' night...\n",
      "8374  id03410  She was buried not in a vault, but in an ordin...\n",
      "8375  id04537  In company with this sprightly and clever Gree...\n",
      "8376  id26628  In this unnerved in this pitiable condition I ...\n",
      "8377  id01586  He was a scoundrel, and I don't blame you for ...\n",
      "8378  id13421  But why should I dwell upon the incidents that...\n",
      "8379  id26084  In the streets were spears of long grass, and ...\n",
      "8380  id05375  When I first sought it, it was the love of vir...\n",
      "8381  id23212  But it is in matters beyond the limits of mere...\n",
      "8382  id15980  \"I may say an excellently well constructed house.\n",
      "8383  id11719  Across a covered bridge one sees a small villa...\n",
      "8384  id13109  You cannot take up a common newspaper in which...\n",
      "8385  id07156  Consoling myself with this reflection, I was m...\n",
      "8386  id04893  Yet we laughed and were merry in our proper wa...\n",
      "8387  id11749         All this is now the fitter for my purpose.\n",
      "8388  id10526                 I fixed myself on a wide solitude.\n",
      "8389  id13477  It is easily understood that what might improv...\n",
      "8390  id13761  Be this as it may, I now began to feel the ins...\n",
      "8391  id04282  Long winded, statistical, and drearily genealo...\n",
      "\n",
      "[8392 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "path_train = 'resource/train.csv'\n",
    "path_test = 'resource/test.csv'\n",
    "\n",
    "train = pandas.read_csv(path_train)\n",
    "test = pandas.read_csv(path_test)\n",
    "\n",
    "train_corpus = train.as_matrix()[:,1]\n",
    "test_corpus = test.as_matrix()[:,1]\n",
    "\n",
    "#print(train_corpus)\n",
    "print(train)\n",
    "#print(test_corpus)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Split text into words, including punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'process', ',', 'however', ',', 'afforded', 'me', 'no', 'means', 'of', 'ascertaining', 'the', 'dimensions', 'of', 'my', 'dungeon', ';', 'as', 'I', 'might', 'make', 'its', 'circuit', ',', 'and', 'return', 'to', 'the', 'point', 'whence', 'I', 'set', 'out', ',', 'without', 'being', 'aware', 'of', 'the', 'fact', ';', 'so', 'perfectly', 'uniform', 'seemed', 'the', 'wall', '.']\n",
      "19579\n",
      "['Still', ',', 'as', 'I', 'urged', 'our', 'leaving', 'Ireland', 'with', 'such', 'inquietude', 'and', 'impatience', ',', 'my', 'father', 'thought', 'it', 'best', 'to', 'yield', '.']\n",
      "8392\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def splitIntoWordList(text):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "# Split the training text word by word, including punctuation as words \n",
    "train_text = list()\n",
    "train_text_length = list()\n",
    "for sentence in train['text']:\n",
    "    l = splitIntoWordList(sentence)\n",
    "    train_text.append(l)\n",
    "    train_text_length.append(len(l))\n",
    "# Split the testing text word by word, including punctuation as words\n",
    "test_text = list()\n",
    "test_text_length = list()\n",
    "for sentence in test['text']:\n",
    "    l = splitIntoWordList(sentence)\n",
    "    test_text.append(l)\n",
    "    test_text_length.append(len(l))\n",
    "\n",
    "print(train_text[0])\n",
    "print(len(train_text_length))    \n",
    "print(test_text[0])\n",
    "print(len(test_text_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "Find out how long sentences are and add to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['length'] = train_text_length\n",
    "test['length'] = test_text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19579.000000\n",
       "mean        30.570611\n",
       "std         21.058091\n",
       "min          4.000000\n",
       "25%         17.000000\n",
       "50%         26.000000\n",
       "75%         39.000000\n",
       "max        875.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a maximum sentence length of 50 words. Sentence with less will be padded, any sentence with more will be truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding of Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EAP' 'HPL' 'EAP' ..., 'EAP' 'EAP' 'HPL']\n",
      "[[ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# One hot encoding of labels\n",
    "train_labels = train.as_matrix()[:,2]\n",
    "train_targets = np.zeros(train_labels.shape)\n",
    "for idx, label in enumerate(train_labels):\n",
    "    if 'EAP' == label:\n",
    "        train_targets[idx] = 0\n",
    "    elif 'HPL' == label:\n",
    "        train_targets[idx] = 1\n",
    "    elif 'MWS' == label:\n",
    "        train_targets[idx] = 2\n",
    "    else:\n",
    "        raise ValueError(\"EAP, HPL or MWS is not in label\")\n",
    "    \n",
    "\n",
    "# One-hot encode\n",
    "encoded_train_targets = to_categorical(train_targets)\n",
    "\n",
    "print(train_labels)\n",
    "print(encoded_train_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.'\n",
      " 'It never once occurred to me that the fumbling might be a mere mistake.'\n",
      " 'In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.'\n",
      " ...,\n",
      " 'Mais il faut agir that is to say, a Frenchman never faints outright.'\n",
      " 'For an item of news like this, it strikes us it was very coolly received.\"'\n",
      " 'He laid a gnarled claw on my shoulder, and it seemed to me that its shaking was not altogether that of mirth.']\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 100)\n",
      "(8392, 100)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(train_corpus)\n",
    "# Break text into sequences and assign integer id to each word in sequence\n",
    "encoded_train_text = t.texts_to_sequences(train_corpus)\n",
    "# Pad text to a max length of sequence_length\n",
    "padded_encoded_train_text = pad_sequences(encoded_train_text, maxlen=sequence_length, padding='post')\n",
    "\n",
    "# Do the same for the test data\n",
    "encoded_test_text = t.texts_to_sequences(test_corpus)\n",
    "padded_encoded_test_text = pad_sequences(encoded_test_text, maxlen=sequence_length, padding='post')\n",
    "\n",
    "print(padded_encoded_train_text.shape)\n",
    "print(padded_encoded_test_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' \n",
    "# Split data into training and validation sets\n",
    "indices = np.arange(padded_encoded_train_text.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "shuffled_padded_encoded_train_text = padded_encoded_train_text[indices]\n",
    "shuffled_encoded_train_targets = encoded_train_targets[indices]\n",
    "\n",
    "VALIDATION_SPLIT = 0.2\n",
    "nb_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = shuffled_padded_encoded_train_text[:-nb_validation_samples]\n",
    "y_train = shuffled_encoded_train_targets[:-nb_validation_samples]\n",
    "x_val = shuffled_padded_encoded_train_text[-nb_validation_samples:]\n",
    "y_val = shuffled_encoded_train_targets[-nb_validation_samples:]\n",
    "'''\n",
    "x_train = padded_encoded_train_text\n",
    "y_train = encoded_train_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12678 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "\n",
    "# f = open('resource/tr_data_embeddings.txt',encoding='utf8')\n",
    "f = open('resource/glovew2v.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimensionality of each embedding\n",
    "embedding_size = len(embeddings_index.get('the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25944\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(t.word_index) + 1\n",
    "print(vocabulary_size)\n",
    "\n",
    "# create a embedding matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_size))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "from keras.layers import Embedding\n",
    "\n",
    "# Input shape to embedding layer: 2D tensor - (batch_size, sequence_length)\n",
    "# Output shape of embedding layer: 3D tensor - (batch_size, sequence_length, output_dim)\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                           embedding_size,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=sequence_length,\n",
    "                           trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "### Single Layer Multi Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EAP, HPL or MWS\n",
    "num_classes= 3\n",
    "# Filter Sizes\n",
    "filter_sizes = [2,3]\n",
    "# The number of filters for each filter size that runs through the whole sentence\n",
    "num_filters = 250\n",
    "batch_size = 64\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders\n",
    "\n",
    "The input images x will consist of a 2d tensor of floating point numbers. Here we assign it a shape of [None, 784], where 784 is the dimensionality of a single flattened 28 by 28 pixel MNIST image, and None indicates that the first dimension, corresponding to the batch size, can be of any size. The target output classes y_ will also consist of a 2d tensor, where each row is a one-hot 10-dimensional vector indicating which digit class (zero through nine) the corresponding MNIST image belongs to.\n",
    "\n",
    "The shape argument to placeholder is optional, but it allows TensorFlow to automatically catch bugs stemming from inconsistent tensor shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_x = tf.placeholder(tf.float32, shape=[None, sequence_length])\n",
    "input_y = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Initialization\n",
    "To create this model, we're going to need to create a lot of weights and biases. One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons\". Instead of doing this repeatedly while we build the model, let's create two handy functions to do it for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Pooling\n",
    "Our convolutions uses a stride of one and are zero padded so that the output is the same size as the input. Our pooling is plain old max pooling over 2x2 blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W is our pre-trained embedding matrix. embedding_layer() creates the actual embedding operation. The result of the embedding operation is a 3-dimensional tensor of shape [batch_size, sequence_length, embedding_size].\n",
    "\n",
    "TensorFlow’s convolutional conv2d operation expects a 4-dimensional tensor with dimensions corresponding to batch, width, height and channel. The result of our embedding doesn’t contain the channel dimension, so we add it manually, leaving us with a layer of shape [None, sequence_length, embedding_size, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded_chars = embedding_layer(input_x)\n",
    "\n",
    "embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    # Convolution Layer\n",
    "    filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "    b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "    conv = tf.nn.conv2d(\n",
    "        embedded_chars_expanded,\n",
    "        W,\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding=\"VALID\",\n",
    "        name=\"conv\")\n",
    "    # Apply nonlinearity\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "    # Max-pooling over the outputs\n",
    "    pooled = tf.nn.max_pool(\n",
    "        h,\n",
    "        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "        strides=[1, 1, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool\")\n",
    "    pooled_outputs.append(pooled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'pool_11:0' shape=(?, 1, 1, 250) dtype=float32>, <tf.Tensor 'pool_12:0' shape=(?, 1, 1, 250) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "print(pooled_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the pooled features\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "h_pool = tf.concat(pooled_outputs, len(filter_sizes))\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "Dropout is the perhaps most popular method to regularize convolutional neural networks. The idea behind dropout is simple. A dropout layer stochastically “disables” a fraction of its neurons. This prevent neurons from co-adapting and forces them to learn individually useful features. The fraction of neurons we keep enabled is defined by the dropout_keep_prob input to our network. We set this to something like 0.5 during training, and to 1 (disable dropout) during evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add drop out\n",
    "h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readout Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "\n",
    "raw_scores = tf.matmul(h_drop, W) + b\n",
    "normalized_scores = tf.nn.softmax(raw_scores)\n",
    "predictions = tf.argmax(raw_scores, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "losses = tf.nn.softmax_cross_entropy_with_logits(logits=raw_scores, labels=input_y)\n",
    "cross_entropy = tf.reduce_mean(losses)\n",
    "\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Started\n"
     ]
    }
   ],
   "source": [
    "probabilities = np.zeros((padded_encoded_test_text.shape[0], 3))\n",
    "print(\"Training Started\")\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), batch_size, num_epochs)\n",
    "    \n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        #if i % 10 == 0:\n",
    "            #train_accuracy = accuracy.eval(feed_dict={input_x: x_batch, input_y: y_batch, dropout_keep_prob: 1.0})\n",
    "            #print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "        train_step.run(feed_dict={input_x: x_batch, input_y: y_batch, dropout_keep_prob: 1.0})\n",
    "    \n",
    "    probabilities = sess.run(normalized_scores, feed_dict={input_x: padded_encoded_test_text, dropout_keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id02310',  0.01351589,  0.00783568,   9.78648365e-01)\n",
      " ('id24541',  0.89418548,  0.0923472 ,   1.34672942e-02)\n",
      " ('id00134',  0.14546807,  0.84682101,   7.71088572e-03) ...,\n",
      " ('id13477',  0.9599672 ,  0.02746166,   1.25710787e-02)\n",
      " ('id13761',  0.34083471,  0.02632907,   6.32836163e-01)\n",
      " ('id04282',  0.01088301,  0.98880708,   3.09838913e-04)]\n"
     ]
    }
   ],
   "source": [
    "test_id = test.as_matrix()[:,0]\n",
    "out = np.rec.fromarrays([test_id, probabilities[:,0], probabilities[:,1], probabilities[:,2]])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Produce csv\n",
    "import csv\n",
    "with open('glove_tf1C_10_64_23.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_NONE)\n",
    "    wr.writerow(('id', 'EAP','HPL','MWS'))\n",
    "    wr.writerows(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
