{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "np.set_printoptions(15, suppress=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = 'resource/train.csv'\n",
    "path_test = 'resource/test.csv'\n",
    "\n",
    "train = pandas.read_csv(path_train)\n",
    "test = pandas.read_csv(path_test)\n",
    "\n",
    "train_corpus = train.as_matrix()[:,1]\n",
    "test_corpus = test.as_matrix()[:,1]\n",
    "\n",
    "#print(train_corpus)\n",
    "#print(train)\n",
    "#print(test_corpus)\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Split text into words, including punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def splitIntoWordList(text):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text)\n",
    "\n",
    "# Split the training text word by word, including punctuation as words \n",
    "train_text = list()\n",
    "train_text_length = list()\n",
    "for sentence in train['text']:\n",
    "    l = splitIntoWordList(sentence)\n",
    "    train_text.append(l)\n",
    "    train_text_length.append(len(l))\n",
    "# Split the testing text word by word, including punctuation as words\n",
    "test_text = list()\n",
    "test_text_length = list()\n",
    "for sentence in test['text']:\n",
    "    l = splitIntoWordList(sentence)\n",
    "    test_text.append(l)\n",
    "    test_text_length.append(len(l))\n",
    "\n",
    "#print(train_text[0])\n",
    "#print(len(train_text_length))    \n",
    "#print(test_text[0])\n",
    "#print(len(test_text_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "Find out how long sentences are and add to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['length'] = train_text_length\n",
    "test['length'] = test_text_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    19579.000000\n",
       "mean        30.570611\n",
       "std         21.058091\n",
       "min          4.000000\n",
       "25%         17.000000\n",
       "50%         26.000000\n",
       "75%         39.000000\n",
       "max        875.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a maximum sentence length of 50 words. Sentence with less will be padded, any sentence with more will be truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sequence_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding of Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# One hot encoding of labels\n",
    "train_labels = train.as_matrix()[:,2]\n",
    "train_targets = np.zeros(train_labels.shape)\n",
    "for idx, label in enumerate(train_labels):\n",
    "    if 'EAP' == label:\n",
    "        train_targets[idx] = 0\n",
    "    elif 'HPL' == label:\n",
    "        train_targets[idx] = 1\n",
    "    elif 'MWS' == label:\n",
    "        train_targets[idx] = 2\n",
    "    else:\n",
    "        raise ValueError(\"EAP, HPL or MWS is not in label\")\n",
    "    \n",
    "\n",
    "# One-hot encode\n",
    "onehot_train_labels = to_categorical(train_targets)\n",
    "\n",
    "#print(train_labels)\n",
    "#print(onehot_train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.'\n",
      " 'It never once occurred to me that the fumbling might be a mere mistake.'\n",
      " 'In his left hand was a gold snuff box, from which, as he capered down the hill, cutting all manner of fantastic steps, he took snuff incessantly with an air of the greatest possible self satisfaction.'\n",
      " ...,\n",
      " 'Mais il faut agir that is to say, a Frenchman never faints outright.'\n",
      " 'For an item of news like this, it strikes us it was very coolly received.\"'\n",
      " 'He laid a gnarled claw on my shoulder, and it seemed to me that its shaking was not altogether that of mirth.']\n"
     ]
    }
   ],
   "source": [
    "print(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(train_corpus)\n",
    "# Break text into sequences and assign integer id to each word in sequence\n",
    "encoded_train_text = t.texts_to_sequences(train_corpus)\n",
    "# Pad text to a max length of sequence_length\n",
    "padded_encoded_train_text = pad_sequences(encoded_train_text, maxlen=sequence_length, padding='post')\n",
    "\n",
    "# Do the same for the test data\n",
    "encoded_test_text = t.texts_to_sequences(test_corpus)\n",
    "padded_encoded_test_text = pad_sequences(encoded_test_text, maxlen=sequence_length, padding='post')\n",
    "\n",
    "#print(padded_encoded_train_text[0])\n",
    "#print(padded_encoded_train_text.shape)\n",
    "#print(padded_encoded_test_text[0])\n",
    "#print(padded_encoded_test_text.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12678 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('resource/glovew2v.txt',encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimensionality of each embedding\n",
    "embedding_size = len(embeddings_index.get('the'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(t.word_index) + 1\n",
    "\n",
    "# create a embedding matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocabulary_size, embedding_size))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25944\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "\n",
    "print(vocabulary_size)\n",
    "\n",
    "# Input shape to embedding layer: 2D tensor - (batch_size, sequence_length)\n",
    "# Output shape of embedding layer: 3D tensor - (batch_size, sequence_length, output_dim)\n",
    "embedding_layer = Embedding(vocabulary_size,\n",
    "                           embedding_size,\n",
    "                           weights=[embedding_matrix],\n",
    "                           input_length=sequence_length,\n",
    "                           trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = padded_encoded_train_text\n",
    "y_train = onehot_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100)\n",
      "(?, 100, 300)\n",
      "Epoch 1/1\n",
      "19579/19579 [==============================] - 89s - loss: 0.6163 - acc: 0.7347    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbbaf4a3cc0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Conv1D, MaxPooling1D, Input\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "\n",
    "num_classes = 3\n",
    "batch_size = 64\n",
    "num_filters = 250\n",
    "filter_size = 3\n",
    "num_epochs = 1\n",
    "\n",
    "sequence_input = Input(shape=(sequence_length,), dtype='int32')\n",
    "print(sequence_input.shape)\n",
    "\n",
    "#sequence_input = padded_encoded_train_text\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "print(embedded_sequences.shape)\n",
    "\n",
    "\n",
    "x = Conv1D(num_filters, filter_size, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(filter_size)(x)\n",
    "\n",
    "x = Conv1D(num_filters, filter_size, activation='relu')(x)\n",
    "x = MaxPooling1D(filter_size)(x)\n",
    "\n",
    "x = Conv1D(num_filters, filter_size, activation='relu')(x)\n",
    "x = MaxPooling1D(filter_size)(x)  # global max pooling\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(batch_size, activation='relu')(x)\n",
    "preds = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# happy learning!\n",
    "model.fit(x_train, y_train, validation_split=0.0, shuffle=True,\n",
    "          epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "propabilities = model.predict(x=padded_encoded_test_text, batch_size=batch_size, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8392, 3)\n"
     ]
    }
   ],
   "source": [
    "print(propabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = test.as_matrix()[:,0]\n",
    "#print(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id02310',  0.068037919700146,  0.034220259636641,  0.897741794586182)\n",
      " ('id24541',  0.870315730571747,  0.11020290106535 ,  0.019481308758259)\n",
      " ('id00134',  0.221613869071007,  0.743544578552246,  0.034841556102037)\n",
      " ...,\n",
      " ('id13477',  0.874395072460175,  0.044355805963278,  0.081249132752419)\n",
      " ('id13761',  0.062903754413128,  0.043545249849558,  0.893551051616669)\n",
      " ('id04282',  0.058063514530659,  0.935851335525513,  0.006085207220167)]\n"
     ]
    }
   ],
   "source": [
    "# Format csv output\n",
    "#idx = np.arange(test_label.size, dtype=np.int16)\n",
    "out = np.rec.fromarrays([test_id, propabilities[:,0], propabilities[:,1], propabilities[:,2]])\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce csv\n",
    "with open('glove_keras3C_1_64_3.csv', 'w') as myfile:\n",
    "    wr = csv.writer(myfile, quoting=csv.QUOTE_NONE)\n",
    "    wr.writerow(('id', 'EAP','HPL','MWS'))\n",
    "    wr.writerows(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
